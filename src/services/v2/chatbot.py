import json
from src.models.gemma_3_4b import call_vllm_api
from src.vector_db.vector_searcher import search_similar_documents, get_user_joined_group_ids
from src.core.ai_logger import get_ai_logger

ai_logger = get_ai_logger()

import re

async def handle_combined_question(
    answer: str | None,
    user_id: str,
    session_id: str,
    debug_mode: bool = True
) -> dict:
    if debug_mode:
        ai_logger.info("[Chatbot] Debug ëª¨ë“œ: ê³ ì • ì§ˆë¬¸ ë°˜í™˜", extra={"user_id": user_id, "session_id": session_id})
        return {
            "question": "ì–´ë–¤ í™œë™ì„ ì¢‹ì•„í•˜ì‹œë‚˜ìš”?",
            "options": ["ìš´ë™", "ìŠ¤í„°ë””", "ë´‰ì‚¬í™œë™", "ê²Œì„"]
        }

    prompt = generate_combined_prompt(answer)
    ai_logger.info("[Chatbot] ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸", extra={"prompt": prompt})

    try:
        raw_response = await call_vllm_api(prompt)
        ai_logger.info("[Chatbot] ì›ì‹œ ì‘ë‹µ", extra={"response": raw_response})

        # âœ… JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì •ê·œì‹ ê¸°ë°˜)
        json_match = re.search(r"\{[\s\S]+?\}", raw_response)
        if not json_match:
            raise ValueError("JSON ë¶€ë¶„ ì¶”ì¶œ ì‹¤íŒ¨")

        cleaned = json_match.group(0)
        parsed = json.loads(cleaned)

        question = parsed.get("question", "").strip()
        options = parsed.get("options", [])

        if not question or not isinstance(options, list) or len(options) < 2:
            ai_logger.warning("[Chatbot] ìƒì„±ëœ ì§ˆë¬¸ í˜•ì‹ ì´ìƒ â†’ fallback ì ìš©", extra={
                "question": question, "options": options
            })
            raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë³´ê¸° ìƒì„± ì‹¤íŒ¨")

        ai_logger.info("[Chatbot] ì§ˆë¬¸ ìƒì„± ì„±ê³µ", extra={
            "user_id": user_id,
            "session_id": session_id,
            "question": question,
            "options": options
        })

        return {
            "question": question,
            "options": options
        }

    except Exception as e:
        ai_logger.warning("[Chatbot] ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨", extra={
            "user_id": user_id,
            "session_id": session_id,
            "error": str(e)
        })
        return {
            "question": "ìƒˆë¡œìš´ ëª¨ì„ì„ ì°¾ê¸° ìœ„í•´ ì–´ë–¤ í™œë™ì„ ì„ í˜¸í•˜ì‹œë‚˜ìš”?",
            "options": ["ë¬¸í™” ì²´í—˜", "ìš´ë™", "ìŠ¤í„°ë””", "ë´‰ì‚¬"]
        }

def generate_combined_prompt(previous_answer: str | None) -> str:
    if previous_answer:
        context = f'"{previous_answer}" ì´ ë‹µë³€ì„ ì°¸ê³ í•´ ì‚¬ìš©ìì—ê²Œ ëª¨ì„ ì·¨í–¥ì„ ë¬»ëŠ” í›„ì† ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”.'
    else:
        context = "ëª¨ì„ ì·¨í–¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ì²« ì§ˆë¬¸ì„ ì‘ì„±í•˜ì„¸ìš”."

    return f"""
{context}

- ì§ˆë¬¸ì€ 100~200ì ì´ë‚´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¬»ëŠ” ë§íˆ¬ë¡œ ì‘ì„±
- AI ìì‹ ì— ëŒ€í•œ ì„¤ëª… ì—†ì´, ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì§ˆë¬¸
- ì„ íƒì§€ëŠ” 1~3ë‹¨ì–´ ì´ë‚´ë¡œ 4ê°œ ì‘ì„±

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì¶œë ¥:
{{
  "question": "...",
  "options": ["...", "...", "...", "..."]
}}
""".strip()


async def handle_answer_analysis(
    messages: list[dict],
    user_id: str,
    session_id: str,
    debug_mode: bool = True
) -> dict:
    if not messages:
        ai_logger.warning("[ì¶”ì²œ] ë¹ˆ ë©”ì‹œì§€ ìˆ˜ì‹ ", extra={"session_id": session_id})
        return {
            "groupId": -1,
            "reason": "ëŒ€í™” ë‚´ìš©ì´ ë¶€ì¡±í•˜ì—¬ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    combined_text = "\n".join([f"{m['role']}: {m['text']}" for m in messages])
    ai_logger.info("[ì¶”ì²œ] ë©”ì‹œì§€ ë³‘í•© ì™„ë£Œ", extra={"session_id": session_id})

    try:
        joined_ids = get_user_joined_group_ids(user_id)
    except Exception:
        joined_ids = set()
        ai_logger.warning("[ì¶”ì²œ] ìœ ì € ì°¸ì—¬ ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨", extra={"session_id": session_id})

    results = search_similar_documents(combined_text, top_k=10)
    filtered = [
        r for r in results
        if r.get("metadata", {}).get("groupId") not in joined_ids
        and r.get("metadata", {}).get("groupId") is not None
    ]

    if not filtered:
        return {
            "groupId": -1,
            "reason": "ì¶”ì²œ ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ëª¨ì„ì´ ì•„ì§ ì—†ì–´ìš”. ì§ì ‘ ë¹„ìŠ·í•œ ëª¨ì„ì„ ì—´ì–´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
        }

    top_result = filtered[0]
    group_id = int(top_result["metadata"]["groupId"])
    group_text = top_result["text"]

    if debug_mode:
        ai_logger.info("[ì¶”ì²œ] Debug ëª¨ë“œ: ì„¤ëª… ìƒëµ", extra={"group_id": group_id})
        return {
            "groupId": group_id,
            "reason": "ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ëª¨ì„ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
        }

    try:
        reason = await generate_explaination(messages, group_text)
        ai_logger.info("[ì¶”ì²œ] ì¶”ì²œ ì‚¬ìœ  ìƒì„± ì„±ê³µ", extra={"group_id": group_id})
    except Exception as e:
        reason = "ì´ ëª¨ì„ì€ ë‹¹ì‹ ì˜ ëŒ€í™” ë‚´ìš©ê³¼ ê°€ì¥ ì˜ ì–´ìš¸ë ¤ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
        ai_logger.warning("[ì¶”ì²œ] ì¶”ì²œ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨", extra={"group_id": group_id, "error": str(e)})

    return {
        "groupId": group_id,
        "reason": reason
    }

async def generate_explaination(messages: list[dict], group_text: str, debug: bool = True) -> str:
    conversation = "\n".join([f"{m['role']}: {m['text']}" for m in messages])

    prompt = f"""
    ë‹¹ì‹ ì€ ëª¨ì„ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤.

    ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:
    {conversation}

    ì¶”ì²œí•  ëª¨ì„ ì •ë³´:
    {group_text.strip()}

    ì´ ëª¨ì„ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì´ìœ ë¥¼ **ë§ˆí¬ë‹¤ìš´ í˜•ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì•„ë˜ ì¡°ê±´ì„ ì§€í‚¤ì„¸ìš”:

    - ìµœëŒ€ 300ì
    - ì œëª© ìŠ¤íƒ€ì¼(ì˜ˆ: `###`, `**`)ì„ í™œìš©í•´ **ëª¨ì„ ì´ë¦„**ì„ ê°•ì¡°í•˜ì„¸ìš”
    - ì¤„ë°”ê¿ˆ(`\\n` ë˜ëŠ” ë¹ˆ ì¤„)ì„ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”
    - ë¦¬ìŠ¤íŠ¸(`-`) ë˜ëŠ” í•˜ì´ë¼ì´íŠ¸(`**`)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”
    - ì„¤ëª…ì€ 300ì ì´ë‚´ë¡œ, í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ    
    - í…ìŠ¤íŠ¸ ì„¤ëª…ë§Œ ì¶œë ¥ (JSON, ë”°ì˜´í‘œ, ë¦¬ìŠ¤íŠ¸ ë“± X)
    - ë¬¸ì¥ì€ í•˜ë‚˜ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë©°, ë°˜ë³µ ì—†ì´ í•µì‹¬ë§Œ ë‹´ì„ ê²ƒ
    - "AI:", "ì„¤ëª…:", "- " ê°™ì€ í¬ë§·ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

    ë°”ë¡œ ì•„ë˜ì— ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”.
    """.strip()

    try:
        explanation = await call_vllm_api(prompt, max_tokens=400)
        if debug:
            print("ğŸ“¦ ìƒì„±ëœ ì¶”ì²œ ì„¤ëª…:\n", explanation)
        return explanation.strip()
    except Exception as e:
        print(f"[â—ï¸generate_explaination ì—ëŸ¬] {e}")
        return "ì¶”ì²œ ì‚¬ìœ ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

import json
import re
from src.core.ai_logger import get_ai_logger
from src.models.gemma_3_4b import call_vllm_api
from src.vector_db.vector_searcher import search_similar_documents, get_user_joined_group_ids
from src.core.chat_cache import get_session_history
from src.core.similarity_filter import is_similar_to_any  # ìœ ì‚¬ ì§ˆë¬¸ ë¹„êµ

ai_logger = get_ai_logger()


async def handle_combined_question(
    answer: str | None,
    user_id: str,
    session_id: str,
    debug_mode: bool = False
) -> dict:
    history = get_session_history(session_id)

    previous_ai_messages = [m["content"] for m in history.get_messages() if m["role"] == "AI"]
    previous_question = previous_ai_messages[-1] if previous_ai_messages else None

    if debug_mode:
        question = "ì–´ë–¤ í™œë™ì„ ì¢‹ì•„í•˜ì‹œë‚˜ìš”?"
        history.add_ai_message(question)
        return {
            "question": question,
            "options": ["ìš´ë™", "ìŠ¤í„°ë””", "ë´‰ì‚¬í™œë™", "ê²Œì„"]
        }

    prompt = generate_combined_prompt(answer, previous_question)
    ai_logger.info("[Chatbot] ì§ˆë¬¸ ìƒì„± í”„ë¡¬í”„íŠ¸", extra={"prompt": prompt})

    try:
        raw_response = await call_vllm_api(prompt)
        ai_logger.info("[Chatbot] ì›ì‹œ ì‘ë‹µ", extra={"response": raw_response})

        json_match = re.search(r"\{[\s\S]+?\}", raw_response)
        if not json_match:
            raise ValueError("JSON ë¶€ë¶„ ì¶”ì¶œ ì‹¤íŒ¨")

        cleaned = json_match.group(0)
        parsed = json.loads(cleaned)

        question = parsed.get("question", "").strip()
        options = parsed.get("options", [])

        if not question or not isinstance(options, list) or len(options) < 2:
            raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë³´ê¸° ìƒì„± ì‹¤íŒ¨")

        past_questions = [m["content"] for m in history.get_messages() if m["role"] == "AI"]
        if is_similar_to_any(question, past_questions):
            ai_logger.info("[Chatbot] ìœ ì‚¬ ì§ˆë¬¸ ê°ì§€ â†’ fallback ì§ˆë¬¸ ë°˜í™˜")
            fallback_q = "ë‹¤ë¥¸ ì‚¬ëŒê³¼ í•¨ê»˜ í•˜ê³  ì‹¶ì€ í™œë™ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            return {
                "question": fallback_q,
                "options": ["ë¬¸í™” ì²´í—˜", "ìš´ë™", "ìŠ¤í„°ë””", "ë´‰ì‚¬"]
            }

        history.add_ai_message(question)

        return {
            "question": question,
            "options": options
        }

    except Exception as e:
        ai_logger.warning("[Chatbot] ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨", extra={
            "user_id": user_id,
            "session_id": session_id,
            "error": str(e)
        })
        return {
            "question": "ìƒˆë¡œìš´ ëª¨ì„ì„ ì°¾ê¸° ìœ„í•´ ì–´ë–¤ í™œë™ì„ ì„ í˜¸í•˜ì‹œë‚˜ìš”?",
            "options": ["ë¬¸í™” ì²´í—˜", "ìš´ë™", "ìŠ¤í„°ë””", "ë´‰ì‚¬"]
        }


def generate_combined_prompt(previous_answer: str | None, previous_question: str | None = None) -> str:
    context_lines = []

    if previous_question:
        context_lines.append(f"ì´ì „ ì§ˆë¬¸: \"{previous_question}\"")
    if previous_answer:
        context_lines.append(f"ì‚¬ìš©ì ë‹µë³€: \"{previous_answer}\"")

    if context_lines:
        context = "\n".join(context_lines) + "\nì´ ì •ë³´ë¥¼ ì°¸ê³ í•´ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."
    else:
        context = "ì‚¬ìš©ìì˜ ëª¨ì„ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ ì²« ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”."

    return f"""
{context}

- ì§ˆë¬¸ì€ 75~120ì ì´ë‚´ì˜ ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”ì²´ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- AIì— ëŒ€í•œ ì„¤ëª… ì—†ì´, ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ì§ˆë¬¸í•˜ì„¸ìš”.
- ì§ˆë¬¸ ë‚´ìš©ì€ ëª¨ì„ì˜ ì„±ê²©, ë¶„ìœ„ê¸°, ê·œëª¨, ëª©ì  ë“± ì‚¬ìš©ìì—ê²Œ ë§ëŠ” 'ëª¨ì„ ìœ í˜•'ì„ íŒŒì•…í•˜ëŠ” ë° ì§‘ì¤‘í•˜ì„¸ìš”.
- ì„ íƒì§€ëŠ” 4ê°œ ì‘ì„±í•˜ì„¸ìš”.
- ê° ì„ íƒì§€ëŠ” 1-3ê°œ ë‹¨ì–´ë¡œ êµ¬ì„±í•˜ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:
{{
  "question": "...",
  "options": ["...", "...", "...", "..."]
}}
""".strip()


async def handle_answer_analysis(
    messages: list[dict],
    user_id: str,
    session_id: str,
    debug_mode: bool = False
) -> dict:
    if not messages:
        ai_logger.warning("[ì¶”ì²œ] ë¹ˆ ë©”ì‹œì§€ ìˆ˜ì‹ ", extra={"session_id": session_id})
        return {
            "groupId": -1,
            "reason": "ëŒ€í™” ë‚´ìš©ì´ ë¶€ì¡±í•˜ì—¬ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    combined_text = "\n".join([f"{m['role']}: {m['text']}" for m in messages])
    ai_logger.info("[ì¶”ì²œ] ë©”ì‹œì§€ ë³‘í•© ì™„ë£Œ", extra={"session_id": session_id})

    try:
        joined_ids = get_user_joined_group_ids(user_id)
    except Exception:
        joined_ids = set()
        ai_logger.warning("[ì¶”ì²œ] ìœ ì € ì°¸ì—¬ ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨", extra={"session_id": session_id})

    results = search_similar_documents(combined_text, top_k=10)
    filtered = [
        r for r in results
        if r.get("metadata", {}).get("groupId") not in joined_ids
        and r.get("metadata", {}).get("groupId") is not None
    ]

    if not filtered:
        return {
            "groupId": -1,
            "reason": "ì¶”ì²œ ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ëª¨ì„ì´ ì•„ì§ ì—†ì–´ìš”. ì§ì ‘ ë¹„ìŠ·í•œ ëª¨ì„ì„ ì—´ì–´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
        }

    top_result = filtered[0]
    group_id = int(top_result["metadata"]["groupId"])
    group_text = top_result["text"]

    try:
        reason = await generate_explaination(messages, group_text)
        ai_logger.info("[ì¶”ì²œ] ì¶”ì²œ ì‚¬ìœ  ìƒì„± ì„±ê³µ", extra={"group_id": group_id})
    except Exception as e:
        reason = "ì´ ëª¨ì„ì€ ë‹¹ì‹ ì˜ ëŒ€í™” ë‚´ìš©ê³¼ ê°€ì¥ ì˜ ì–´ìš¸ë ¤ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
        ai_logger.warning("[ì¶”ì²œ] ì¶”ì²œ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨", extra={"group_id": group_id, "error": str(e)})

    get_session_history(session_id).clear()

    return {
        "groupId": group_id,
        "reason": reason
    }


async def generate_explaination(messages: list[dict], group_text: str, debug: bool = True) -> str:
    conversation = "\n".join([f"{m['role']}: {m['text']}" for m in messages])

    prompt = f"""
    ë‹¹ì‹ ì€ ëª¨ì„ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤.

    ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤:
    {conversation}

    ì¶”ì²œí•  ëª¨ì„ ì •ë³´:
    {group_text.strip()}

    ì´ ëª¨ì„ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì´ìœ ë¥¼ **ë§ˆí¬ë‹¤ìš´ í˜•ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì•„ë˜ ì¡°ê±´ì„ ì§€í‚¤ì„¸ìš”:

    - ì„¤ëª…ì€ 150~270ì ì´ë‚´ë¡œ, í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ    
    - í…ìŠ¤íŠ¸ ì„¤ëª…ë§Œ ì¶œë ¥ (JSON, ë”°ì˜´í‘œ, ë¦¬ìŠ¤íŠ¸ ë“± X)
    - ë¬¸ì¥ì€ í•˜ë‚˜ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë©°, ë°˜ë³µ ì—†ì´ í•µì‹¬ë§Œ ë‹´ì„ ê²ƒ
    - "AI:", "ì„¤ëª…:", "- " ê°™ì€ í¬ë§·ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

    ë°”ë¡œ ì•„ë˜ì— ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”.
    """.strip()

    explanation = await call_vllm_api(prompt, max_tokens=400)
    cleaned = re.sub(
        r"^\s*(?:ì„¤ëª…|ì¶”ì²œ|AI|\[AI\]|ëª¨ì„\s*ì´ë¦„)\s*[:ï¼š-]?\s*",
        "",
        explanation.strip(),
        flags=re.IGNORECASE
    )

    if debug:
        print("ğŸ“¦ ìƒì„±ëœ ì¶”ì²œ ì„¤ëª…:\n", cleaned)

    return cleaned

#removed from prompt:
"""
- ìµœëŒ€ 300ì
- ì œëª© ìŠ¤íƒ€ì¼(ì˜ˆ: `###`, `**`)ì„ í™œìš©í•´ **ëª¨ì„ ì´ë¦„**ì„ ê°•ì¡°í•˜ì„¸ìš”
- ì¤„ë°”ê¿ˆ(`\\n` ë˜ëŠ” ë¹ˆ ì¤„)ì„ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”
- ë¦¬ìŠ¤íŠ¸(`-`) ë˜ëŠ” í•˜ì´ë¼ì´íŠ¸(`**`)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”
"""
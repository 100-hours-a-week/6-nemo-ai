from src.models.gemma_3_4b import call_vllm_api
from src.vector_db.vector_searcher import search_similar_documents, get_user_joined_group_ids
import json
from src.core.ai_logger import get_ai_logger

ai_logger = get_ai_logger()

async def handle_combined_question(
    answer: str | None,
    user_id: str,
    session_id: str,
    debug_mode: bool = True
) -> dict:
    """
    ì´ì „ ë‹µë³€(answer)ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ì—¬
    question + options(answer list)ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """

    if debug_mode:
        ai_logger.info("[Chatbot] Debug ëª¨ë“œ: ê³ ì • ì§ˆë¬¸/ì„ íƒì§€ ë°˜í™˜", extra={
            "user_id": user_id,
            "session_id": session_id
        })
        return {
            "question": "ì–´ë–¤ í™œë™ì„ ì¢‹ì•„í•˜ì‹œë‚˜ìš”?",
            "options": ["ìš´ë™", "ìŠ¤í„°ë””", "ë´‰ì‚¬í™œë™", "ê²Œì„"]
        }

    prompt = generate_combined_prompt(answer)

    try:
        raw_response = await call_vllm_api(prompt)

        # ë¶ˆí•„ìš”í•œ ë§ˆí¬ë‹¤ìš´ ì œê±° ë° ì •ì œ
        cleaned = raw_response.strip().removeprefix("```json").removesuffix("```").strip()
        parsed = json.loads(cleaned)

        question = parsed.get("question", "").strip()
        options = parsed.get("options", [])

        if not question or not isinstance(options, list) or len(options) < 2:
            raise ValueError("ì§ˆë¬¸ ë˜ëŠ” ë³´ê¸° ìƒì„± ì‹¤íŒ¨")

        ai_logger.info("[Chatbot] ì§ˆë¬¸ ìƒì„± ì„±ê³µ", extra={
            "user_id": user_id,
            "session_id": session_id,
            "question": question
        })

        return {
            "question": question,
            "options": options
        }

    except Exception as e:
        ai_logger.warning("[Chatbot] ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨", extra={
            "user_id": user_id,
            "session_id": session_id,
            "error": str(e)
        })

        return {
            "question": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆì–´ìš”.",
            "options": []
        }

def generate_combined_prompt(previous_answer: str | None) -> str:
    """
    ì‚¬ìš©ìì˜ ì´ì „ ì‘ë‹µì´ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ë¬¸ì¥ê³¼ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³ ,
    ì—†ë‹¤ë©´ ëª¨ì„ ì„±í–¥ íŒŒì•…ì„ ìœ„í•œ ì²« ì§ˆë¬¸ì„ êµ¬ì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìƒì„±ëœ ì§ˆë¬¸ì€ ì¶”ì²œí•  ëª¨ì„ì„ ë” ì •í™•íˆ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    """

    if previous_answer:
        intro = (
            f"ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëª¨ì„ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤.\n"
            f"ì‚¬ìš©ìê°€ ì´ì „ì— ì•„ë˜ì™€ ê°™ì´ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤:\n\n"
            f"\"{previous_answer}\"\n\n"
            f"ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ì‚¬ìš©ìì˜ ì„±í–¥ì´ë‚˜ ê´€ì‹¬ì‚¬ë¥¼ íŒŒì•…í•˜ì—¬ ëª¨ì„ì„ ë” ì˜ ì¶”ì²œí•  ìˆ˜ ìˆë„ë¡\n"
            f"ì´ì–´ì§€ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ê³¼ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.\n"
        )
    else:
        intro = (
            "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ëª¨ì„ ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤.\n"
            "ì‚¬ìš©ìê°€ ì²˜ìŒ ì§ˆë¬¸ì— ì‘ë‹µí•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.\n"
            "ì‚¬ìš©ìì˜ ëª¨ì„ ì°¸ì—¬ ì„±í–¥ì´ë‚˜ ê´€ì‹¬ì‚¬ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´\n"
            "ì ì ˆí•œ ì²« ì§ˆë¬¸ì„ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”.\n"
        )

    instruction = (
        "ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n\n"
        "{\n"
        "  \"question\": \"ìì—°ìŠ¤ëŸ¬ìš´ ë„ì… ë¬¸ì¥ê³¼ í•¨ê»˜ ì´ì–´ì§€ëŠ” ì§ˆë¬¸\",\n"
        "  \"options\": [\"ì„ íƒì§€1\", \"ì„ íƒì§€2\", \"ì„ íƒì§€3\", \"ì„ íƒì§€4\"]\n"
        "}\n\n"
        "ì§ˆë¬¸ì€ ì¶”ì²œí•  ëª¨ì„ì„ ê³ ë¥¼ ë•Œ ë„ì›€ì´ ë  ìˆ˜ ìˆë„ë¡\n"
        "ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬, í™œë™ ìŠ¤íƒ€ì¼, ì‹œê°„ëŒ€, ì‚¬ëŒê³¼ì˜ êµë¥˜ ë°©ì‹ ë“±ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤.\n"
        "ì§ˆë¬¸ ê¸¸ì´ëŠ” ì•½ 100ì ì´ë‚´ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n"
        "ë¬¸ì¥ì€ ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²˜ëŸ¼ ì‹œì‘í•˜ê³ , ë§ˆì§€ë§‰ì— ì§ˆë¬¸ìœ¼ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.\n"
        "ì„ íƒì§€ëŠ” 4ê°œë¡œ êµ¬ì„±í•˜ê³ , ê°ê°ì€ ëª…í™•í•˜ê³  ì¤‘ë³µë˜ì§€ ì•Šìœ¼ë©° 4ë‹¨ì–´ ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
    )

    return intro + instruction

async def handle_answer_analysis(
    messages: list[dict],
    user_id: str,
    session_id: str,
    debug_mode: bool = True  # LLM í˜¸ì¶œ ì—†ì´ ê³ ì • ì‚¬ìœ ë§Œ ë°˜í™˜
) -> dict:
    if not messages:
        ai_logger.warning("[ì¶”ì²œ] ë¹ˆ ë©”ì‹œì§€ ìˆ˜ì‹ ", extra={"session_id": session_id})
        return {
            "groupId": -1,
            "reason": "ëŒ€í™” ë‚´ìš©ì´ ë¶€ì¡±í•˜ì—¬ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    # 1. ëŒ€í™” ë‚´ìš© í•©ì¹˜ê¸°
    combined_text = "\n".join([f"{m['role']}: {m['text']}" for m in messages])
    ai_logger.info("[ì¶”ì²œ] ë©”ì‹œì§€ ë³‘í•© ì™„ë£Œ", extra={"session_id": session_id})

    # 2. ì´ë¯¸ ì°¸ì—¬í•œ ê·¸ë£¹ ì œì™¸
    try:
        joined_ids = get_user_joined_group_ids(user_id)
    except Exception:
        joined_ids = set()
        ai_logger.warning("[ì¶”ì²œ] ìœ ì € ì°¸ì—¬ ê·¸ë£¹ ì¡°íšŒ ì‹¤íŒ¨", extra={"session_id": session_id})

    # 3. ìœ ì‚¬ ê·¸ë£¹ ê²€ìƒ‰
    results = search_similar_documents(combined_text, top_k=10)
    filtered = [
        r for r in results
        if r.get("metadata", {}).get("groupId") not in joined_ids
           and r.get("metadata", {}).get("groupId") is not None
    ]

    if not filtered:
        return {
            "groupId": -1,
            "reason": "ì¶”ì²œ ê°€ëŠ¥í•œ ìƒˆë¡œìš´ ëª¨ì„ì´ ì•„ì§ ì—†ì–´ìš”. ì§ì ‘ ë¹„ìŠ·í•œ ëª¨ì„ì„ ì—´ì–´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
        }

    # 4. ì¶”ì²œ ê·¸ë£¹ ì„ íƒ
    top_result = filtered[0]
    group_id = int(top_result["metadata"]["groupId"])
    group_text = top_result["text"]

    # 5. ì„¤ëª… ìƒì„± (LLM í˜¸ì¶œì€ ìƒëµí•˜ê³ , ê³ ì •ëœ ë©”ì‹œì§€ë¡œ ëŒ€ì²´)
    if debug_mode:
        ai_logger.info("[ì¶”ì²œ] Debug ëª¨ë“œ: LLM ì—†ì´ ì„¤ëª… ìƒì„± ìƒëµ", extra={"group_id": group_id})
        return {
            "groupId": group_id,
            "reason": "ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ëª¨ì„ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
        }

    try:
        reason = await generate_explaination(messages, group_text)
        ai_logger.info("[ì¶”ì²œ] ì¶”ì²œ ì‚¬ìœ  ìƒì„± ì™„ë£Œ", extra={"group_id": group_id})
    except Exception:
        reason = "ì´ ëª¨ì„ì€ ë‹¹ì‹ ì˜ ëŒ€í™” ë‚´ìš©ê³¼ ê°€ì¥ ì˜ ì–´ìš¸ë ¤ ì¶”ì²œë“œë¦½ë‹ˆë‹¤."
        ai_logger.warning("[ì¶”ì²œ] ì¶”ì²œ ì‚¬ìœ  ìƒì„± ì‹¤íŒ¨", extra={"group_id": group_id})

    return {
        "groupId": group_id,
        "reason": reason
    }

async def generate_explaination(messages: list[dict], group_text: str, debug: bool = False) -> str:
    """
    ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ + ì¶”ì²œ ê·¸ë£¹ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ
    ì´ ê·¸ë£¹ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì´ìœ ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
    """

    # ëŒ€í™” í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜ (AIë¶€í„° ì‹œì‘í•´ì•¼ í•¨)
    conversation = "\n".join([f"{m['role']}: {m['text']}" for m in messages])

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
ë‹¹ì‹ ì€ ëŒ€í™”í˜• ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ëŒ€í™”ëŠ” 'ai'ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

{conversation}

ê·¸ë¦¬ê³  ì•„ë˜ëŠ” ì¶”ì²œ í›„ë³´ ëª¨ì„ì˜ ì •ë³´ì…ë‹ˆë‹¤:

"{group_text.strip()}"

ìœ„ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ ëª¨ì„ì´ ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ì™€ ì–´ë–»ê²Œ ì˜ ë§ëŠ”ì§€ 300ì ì´ë‚´ë¡œ ê³µê°ì˜ ë§ë¡œ ì‹œì‘í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ë‹¨ìˆœí•œ ë¬¸ì¥ì´ ì•„ë‹Œ, ì‚¬ìš©ìì˜ ëŒ€í™” íë¦„ê³¼ ì—°ê²°ëœ ì¶”ì²œ ì‚¬ìœ ì—¬ì•¼ í•©ë‹ˆë‹¤.

í˜•ì‹:
- ì„¤ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”. "ì¶”ì²œë“œë¦½ë‹ˆë‹¤" ë“±ì˜ ë§ˆë¬´ë¦¬ ë§ì€ í¬í•¨í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤.
- JSON, ë§ˆí¬ë‹¤ìš´ ë“± í¬ë§·ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    """.strip()

    try:
        explanation = await call_vllm_api(prompt, max_tokens=400)

        if debug:
            print("ğŸ“¦ ìƒì„±ëœ ì¶”ì²œ ì„¤ëª…:\n", explanation)

        return explanation.strip()

    except Exception as e:
        print(f"[â—ï¸generate_explaination ì—ëŸ¬] {e}")
        return "ì¶”ì²œ ì‚¬ìœ ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
from transformers import Gemma3ForCausalLM, AutoTokenizer
import torch, httpx
from src.core.ai_logger import get_ai_logger

ai_logger = get_ai_logger()

# model_id = "google/gemma-3-4b-it"
#
# torch.cuda.empty_cache()
# tokenizer = AutoTokenizer.from_pretrained(model_id)
# model = Gemma3ForCausalLM.from_pretrained(model_id).eval()
#
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)

VLLM_API_URL = "http://localhost:8000/generate"

async def generate_explaination(messages: list[dict], group_text: str, debug: bool = False) -> str:
    """
    ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ + ì¶”ì²œ ê·¸ë£¹ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ
    ì´ ê·¸ë£¹ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œ ì´ìœ ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
    """

    # ëŒ€í™” í˜•ì‹ ë¬¸ìì—´ë¡œ ë³€í™˜ (AIë¶€í„° ì‹œì‘í•´ì•¼ í•¨)
    conversation = "\n".join([f"{m['role']}: {m['text']}" for m in messages])

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
ë‹¹ì‹ ì€ ëŒ€í™”í˜• ì¶”ì²œ ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ëŒ€í™”ëŠ” 'ai'ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

{conversation}

ê·¸ë¦¬ê³  ì•„ë˜ëŠ” ì¶”ì²œ í›„ë³´ ëª¨ì„ì˜ ì •ë³´ì…ë‹ˆë‹¤:

"{group_text.strip()}"

ìœ„ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ ëª¨ì„ì´ ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬ì™€ ì–´ë–»ê²Œ ì˜ ë§ëŠ”ì§€ 300ì ì´ë‚´ë¡œ ê³µê°ì˜ ë§ë¡œ ì‹œì‘í•˜ì—¬ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ë‹¨ìˆœí•œ ë¬¸ì¥ì´ ì•„ë‹Œ, ì‚¬ìš©ìì˜ ëŒ€í™” íë¦„ê³¼ ì—°ê²°ëœ ì¶”ì²œ ì‚¬ìœ ì—¬ì•¼ í•©ë‹ˆë‹¤.

í˜•ì‹:
- ì„¤ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”. "ì¶”ì²œë“œë¦½ë‹ˆë‹¤" ë“±ì˜ ë§ˆë¬´ë¦¬ ë§ì€ í¬í•¨í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤.
- JSON, ë§ˆí¬ë‹¤ìš´ ë“± í¬ë§·ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    """.strip()

    try:
        explanation = await call_vllm_api(prompt, max_tokens=300)

        if debug:
            print("ğŸ“¦ ìƒì„±ëœ ì¶”ì²œ ì„¤ëª…:\n", explanation)

        return explanation.strip()

    except Exception as e:
        print(f"[â—ï¸generate_explaination ì—ëŸ¬] {e}")
        return "ì¶”ì²œ ì‚¬ìœ ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

async def call_vllm_api(prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
    payload = {
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(VLLM_API_URL, json=payload)

        response.raise_for_status()
        result = response.json()
        generated = result.get("text", "").strip()

        ai_logger.info("[vLLM] ì‘ë‹µ ìˆ˜ì‹  ì„±ê³µ", extra={
            "length": len(generated)
        })

        return generated

    except Exception as e:
        ai_logger.warning("[vLLM] ì‘ë‹µ ì‹¤íŒ¨", extra={"error": str(e)})
        return "```json\n{\"question\": \"ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨\", \"options\": []}\n```"

# async def local_model_generate(prompt: str, max_new_tokens: int = 512) -> str:
#     inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
#     input_len = inputs["input_ids"].shape[-1]
#
#     with torch.no_grad():
#         outputs = model.generate(
#             **inputs,
#             max_new_tokens=max_new_tokens,
#             do_sample=True,
#             temperature=0.4,
#             top_k=50,
#             top_p=0.8,
#             repetition_penalty=1.1
#         )
#
#     decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return decoded, input_len


# if __name__ == "__main__":
#     import asyncio
#     async def run_test():
#         prompt = "ë”¥ëŸ¬ë‹ ë™ì•„ë¦¬ì— ëŒ€í•œ ì†Œê°œê¸€ì„ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜."
#         result, _ = await local_model_generate(prompt)
#         print(result)
#     asyncio.run(run_test())
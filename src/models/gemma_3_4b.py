from transformers import Gemma3ForCausalLM, AutoTokenizer
import torch, json, re

model_id = "google/gemma-3-4b-it"

torch.cuda.empty_cache()
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = Gemma3ForCausalLM.from_pretrained(model_id).eval()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)


def generate_explaination(user_query: str, group_texts: list[str], max_tokens=500, temp=0.7, debug: bool = False) -> str:
    try:
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¶”ì²œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."}]
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            f"ì‚¬ìš©ìì˜ ìš”ì²­: \"{user_query}\"\n\n"
                            f"ì•„ë˜ëŠ” ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ëª¨ì„ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤:\n\n"
                            + "\n\n".join(f"- {t}" for t in group_texts)
                            + "\n\nê° ëª¨ì„ì´ ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ ì–´ë–¤ ì ì—ì„œ ì˜ ë§ëŠ”ì§€ 300ì ì´ë‚´ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”. ê° ì„¤ëª…ì€ ì´ìœ  ì¤‘ì‹¬ì´ë©° ê³µê°ì˜ ë§ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤."
                        )
                    }
                ]
            }
        ]

        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt"
        ).to(device)

        input_len = inputs["input_ids"].shape[-1]

        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=temp
            )

        decoded = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)

        if debug:
            print(f"ğŸ“ Input Tokens: {input_len}, Output Tokens: {outputs.shape}")
            print(f"ğŸ“¦ ìƒì„±ëœ í…ìŠ¤íŠ¸:\n{decoded}")
            if torch.cuda.is_available():
                print(f"ğŸ§  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")

        return decoded.strip()

    except Exception as e:
        print(f"[â—ï¸generate_explaination ì—ëŸ¬] {e}")
        return "ì¶”ì²œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."


def generate_mcq_questions(max_tokens=500, temp=0.7, debug: bool = False, use_context: bool = True) -> list[dict]:
    if use_context:
        return [
            {
                "question": "ë‹¹ì‹ ì€ ì–´ë–¤ ë¶„ìœ„ê¸°ì˜ ëª¨ì„ì„ ì„ í˜¸í•˜ì‹œë‚˜ìš”?",
                "options": ["ì¡°ìš©í•œ", "í™œê¸°ì°¬", "í¸ì•ˆí•œ", "ìƒˆë¡œìš´ ì‚¬ëŒë“¤ê³¼ ì–´ìš¸ë¦´ ìˆ˜ ìˆëŠ”"]
            },
            {
                "question": "í•˜ë£¨ ì¤‘ ëª¨ì„ì— ì°¸ì—¬í•˜ê³  ì‹¶ì€ ì‹œê°„ëŒ€ëŠ” ì–¸ì œì¸ê°€ìš”?",
                "options": ["ì˜¤ì „", "ì˜¤í›„", "ì €ë…", "ì‹œê°„ ìƒê´€ì—†ìŒ"]
            },
            {
                "question": "ì–´ë–¤ í™œë™ì— ê°€ì¥ í¥ë¯¸ë¥¼ ëŠë¼ì‹œë‚˜ìš”?",
                "options": ["ìš´ë™", "ìŠ¤í„°ë””", "ë¬¸í™” ì²´í—˜", "ìˆ˜ë‹¤ ë‚˜ëˆ„ê¸°"]
            }
        ]

    try:
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì¶”ì²œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."}]
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "ì‚¬ìš©ìì˜ ì„±í–¥ê³¼ ëª¨ì„ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°ê´€ì‹ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”:\n"
                            "- ì§ˆë¬¸ì€ ëª¨ë‘ 'ëª¨ì„' ë˜ëŠ” 'ì‚¬ìš©ì ì„±í–¥'ì— ê´€ë ¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì§ˆë¬¸ì€ 100ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”\n"
                            "- ê° ì§ˆë¬¸ì€ 3~5ê°œì˜ ë³´ê¸°ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ë³´ê¸°ëŠ” 20ì ì´ë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
                            "- JSON í¬ë§·ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n"
                            "[\n"
                            "  {\n"
                            "    \"question\": \"...\",\n"
                            "    \"options\": [\"ì„ íƒì§€1\", \"ì„ íƒì§€2\", ...]\n"
                            "  }\n"
                            "]"
                        )
                    }
                ]
            }
        ]

        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt"
        ).to(device)

        input_len = inputs["input_ids"].shape[-1]

        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                top_k=40
            )

        decoded = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
        cleaned = re.sub(r"```json|```", "", decoded.strip()).strip()

        if debug:
            print(f"ğŸ“¦ ìƒì„±ëœ MCQ ì§ˆë¬¸:\n{cleaned}")

        return json.loads(cleaned)
    except Exception as e:
        print(f"[â—ï¸generate_mcq_questions ì—ëŸ¬] {e}")
        return []

async def local_model_generate(prompt: str, max_new_tokens: int = 512) -> str:
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    input_len = inputs["input_ids"].shape[-1]

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.4,
            top_k=50,
            top_p=0.8,
            repetition_penalty=1.1
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded, input_len


if __name__ == "__main__":
    import asyncio
    async def run_test():
        prompt = "ë”¥ëŸ¬ë‹ ë™ì•„ë¦¬ì— ëŒ€í•œ ì†Œê°œê¸€ì„ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜."
        result, _ = await local_model_generate(prompt)
        print(result)
    asyncio.run(run_test())